import requests
from bs4 import BeautifulSoup
from collections import Counter
from urllib.parse import urlparse
import re

# Define the URL to scrape
url = 'https://smarc.se/'

# Parse the base URL using urlparse
base_url = urlparse(url).netloc

# import list from file of common words
with open('1000-common-english-words.txt', 'r') as f:
    common_words = [l.strip() for l in f.read().splitlines()]

def fetch_and_count_words(url):
    """
    Fetches the HTML content of the given URL, extracts all the words in the HTML body,
    counts their frequency, and returns a Counter object containing the word counts.
    """
    # Send a request to the URL and get the HTML content
    response = requests.get(url)
    html_content = response.content

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')

    # Extract the HTML body
    body = soup.find('body')

    # Extract all the text from the body
    text = body.get_text()

    # Split the text into words
    words = [w.lower() for w in re.findall(r'\b\w+\b', text)]

    # Count the frequency of each word using the Counter class
    word_counts = Counter(words)


    return word_counts

def filter_link(link):
    """
    Returns True if the given link should be visited, False otherwise.
    """
    # Parse the link using urlparse
    try:
        parsed_link = urlparse(link)

        # Skip links that aren't on the same domain or start with #
        if not parsed_link.netloc.endswith(base_url) or parsed_link.path.startswith('#'):
            return False

        return True
    except:
        return False


# Keep track of word counts for all pages
all_word_counts = Counter()

# Send a request to the URL and get the HTML content
response = requests.get(url)
html_content = response.content

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser')

# Find all the links on the page and visit each one
links = set([link.get('href') for link in soup.find_all('a') if filter_link(link.get('href'))])
for i, link in enumerate(links):
    print(f'Visiting link {i + 1}/{len(links)}: {link}')
    # Parse the link using urlparse
    try:
        parsed_link = urlparse(link)
        # Construct the full URL of the link
        full_link = link if parsed_link.scheme else f"{parsed_link._replace(scheme='https').geturl()}"
        all_word_counts += fetch_and_count_words(full_link)
    except:
        continue

# Print the word counts for all pages combined

# Exclude the common English words from the counts
for word in common_words:
    if word in all_word_counts:
        del all_word_counts[word]


# print wordlist to file
with open('wordlist.txt', 'w') as f:
    for (word, _) in all_word_counts.most_common(100):
        f.write(word + '\n')
